{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starter code for open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xPositive = []\n",
    "xNegative = []\n",
    "xSeq = []\n",
    "yContinous = []\n",
    "yDiscrete = []\n",
    "replaceWithNoSpace = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "replaceWithSpace = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "for s in ['pos', 'neg']: \n",
    "    files = glob.glob('./comp-551-imbd-sentiment-classification/train/'+ s +'/*.txt')\n",
    "    files.sort(key = lambda x: int(x.split('/')[-1].split('_')[0]))\n",
    "    for name in files:\n",
    "        yContinous.append(int(name.split('_')[1].replace('.txt', '')))\n",
    "        xSeq.append(int(name.split('/')[-1].split('_')[0]))\n",
    "        if s == 'pos': \n",
    "            yDiscrete.append(1)\n",
    "        else: \n",
    "            yDiscrete.append(0)\n",
    "        with open(name) as f:\n",
    "            txt = f.read()\n",
    "            txt2 = replaceWithNoSpace.sub('', txt.lower())\n",
    "            txtf = replaceWithSpace.sub(' ', txt2)\n",
    "            if s == 'pos': \n",
    "                xPositive.append(txtf)\n",
    "            else: \n",
    "                xNegative.append(txtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[x, yd, yc] for x, yd, yc in zip(xPositive + xNegative, yDiscrete, yContinous)], columns=['text', 'y discrete', 'y continous'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the text into training sets and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 9\n",
    "totalN = df.shape[0]\n",
    "posData = df[0:int(totalN/2)]\n",
    "negData = df[int(totalN/2): totalN]\n",
    "trainPos, testPos = posData[0:11250], posData[11250:12500]\n",
    "trainNeg, testNeg = negData[0:11250], negData[11250:12500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the training sets into 9 subsets. Everytime choose one portion as validation set and the others as training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  y discrete  \\\n",
      "0      bromwell high is a cartoon comedy it ran at th...           1   \n",
      "1      if you like adult comedy cartoons like south p...           1   \n",
      "2      bromwell high is nothing short of brilliant ex...           1   \n",
      "3      all the worlds a stage and its people actors i...           1   \n",
      "4      futz is the only show preserved from the exper...           1   \n",
      "5      i came in in the middle of this film so i had ...           1   \n",
      "6      fair drama love story movie that focuses on th...           1   \n",
      "7      although i didnt like stanley & iris tremendou...           1   \n",
      "8      very good drama although it appeared to have a...           1   \n",
      "9      working class romantic drama from director mar...           1   \n",
      "10     im a male not given to womens movies but this ...           1   \n",
      "11     liked stanley & iris very much acting was very...           1   \n",
      "12     liked stanley & iris very much acting was very...           1   \n",
      "13     the production quality cast premise authentic ...           1   \n",
      "14     this film has a special place in my heart as w...           1   \n",
      "15     i guess if a film has magic i dont need it to ...           1   \n",
      "16     i found this to be a so so romance drama that ...           1   \n",
      "17     this is a complex film that explores the effec...           1   \n",
      "18     `stanley and iris is a heart warming film abou...           1   \n",
      "19     i just read the comments of tomreynolds and fe...           1   \n",
      "20     stanley and iris show the triumph of the human...           1   \n",
      "21     this may not be a memorable classic but it is ...           1   \n",
      "22     if you fast forward through the horrible singi...           1   \n",
      "23     ok so the musical pieces were poorly written a...           1   \n",
      "24     although this was obviously a low budget produ...           1   \n",
      "25     i never saw this when i was a kid so this was ...           1   \n",
      "26     when i was a kid i watched this many times ove...           1   \n",
      "27     emilio miraglios the red queen kills seven tim...           1   \n",
      "28     and heres yet another piece of evidence to cla...           1   \n",
      "29     there are some films that every horror fan owe...           1   \n",
      "...                                                  ...         ...   \n",
      "11220  aim for the top gunbuster is one of those anim...           1   \n",
      "11221  terribly underrated with matt dillon and tom s...           1   \n",
      "11222  very entertaining and a great cast as noted id...           1   \n",
      "11223  tommy jones and matt dillon do the gambling wo...           1   \n",
      "11224  considering the big name cast and lavish produ...           1   \n",
      "11225  what a surprisingly good movie this one turned...           1   \n",
      "11226  star trek hidden frontier will surprise you in...           1   \n",
      "11227  hidden frontiers is more than fan fiction  it ...           1   \n",
      "11228  this is short and to the point the story writi...           1   \n",
      "11229  with the amount of actors they have working on...           1   \n",
      "11230  hidden frontier has been talked about and repo...           1   \n",
      "11231  i really like star trek hidden frontier it is ...           1   \n",
      "11232  okay so the first few seasons took a while to ...           1   \n",
      "11233  i kind of consider myself as the # fan of hidd...           1   \n",
      "11234  this is a brilliant and well made contribution...           1   \n",
      "11235  nicely done and along with new voyages its a g...           1   \n",
      "11236  for the record i am not affiliated with the pr...           1   \n",
      "11237  star trek hidden frontier is a long running in...           1   \n",
      "11238  hidden frontier is a fan made show in the worl...           1   \n",
      "11239  hidden frontier is notable for being the longe...           1   \n",
      "11240  low budget brit pop melodrama focuses on a gir...           1   \n",
      "11241  i acquired this one of my all time favourite f...           1   \n",
      "11242  anyone interested in pop music and not familia...           1   \n",
      "11243  i first saw breaking glass when it was release...           1   \n",
      "11244  one of my best films ever maybe because i was ...           1   \n",
      "11245  i first saw breaking glass in  and thought tha...           1   \n",
      "11246  breaking glass is a film that everyone aspirin...           1   \n",
      "11247  i first saw the film when it landed on us cabl...           1   \n",
      "11248  illudere to delude comes from latin verb luder...           1   \n",
      "11249  i have just watched the whole  episodes on dvd...           1   \n",
      "\n",
      "       y continous  \n",
      "0                9  \n",
      "1                7  \n",
      "2                9  \n",
      "3               10  \n",
      "4                8  \n",
      "5               10  \n",
      "6               10  \n",
      "7                7  \n",
      "8                7  \n",
      "9                7  \n",
      "10               9  \n",
      "11               9  \n",
      "12               9  \n",
      "13               7  \n",
      "14              10  \n",
      "15               7  \n",
      "16               7  \n",
      "17               9  \n",
      "18               7  \n",
      "19              10  \n",
      "20               9  \n",
      "21               7  \n",
      "22               8  \n",
      "23               7  \n",
      "24               8  \n",
      "25               7  \n",
      "26               9  \n",
      "27              10  \n",
      "28              10  \n",
      "29              10  \n",
      "...            ...  \n",
      "11220           10  \n",
      "11221           10  \n",
      "11222            8  \n",
      "11223            9  \n",
      "11224            8  \n",
      "11225           10  \n",
      "11226            7  \n",
      "11227            9  \n",
      "11228           10  \n",
      "11229           10  \n",
      "11230           10  \n",
      "11231           10  \n",
      "11232           10  \n",
      "11233           10  \n",
      "11234           10  \n",
      "11235            8  \n",
      "11236            8  \n",
      "11237            8  \n",
      "11238            7  \n",
      "11239            8  \n",
      "11240            7  \n",
      "11241           10  \n",
      "11242            9  \n",
      "11243           10  \n",
      "11244           10  \n",
      "11245           10  \n",
      "11246            8  \n",
      "11247            8  \n",
      "11248           10  \n",
      "11249            7  \n",
      "\n",
      "[11250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from numpy import matrix\n",
    "import numpy as np\n",
    "\n",
    "# the output will be a list of size 9\n",
    "trainPosMat = matrix(trainPos)\n",
    "trainNegMat = matrix(trainNeg)\n",
    "kf = KFold(n_splits = fold)\n",
    "kf.get_n_splits(trainPos)\n",
    "print(trainPos)\n",
    "# iteratively produce the train set and the validation set\n",
    "for train_index, validation_index in kf.split(trainPosMat):\n",
    "    trainP, validationP = trainPosMat[train_index], trainPosMat[validation_index]\n",
    "    trainN, validationN = trainNegMat[train_index], trainNegMat[validation_index]\n",
    "    trainBoth = np.concatenate((trainP, trainN))\n",
    "    validationBoth = np.concatenate((validationP, validationN))\n",
    "    # call the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'i', 'great', 'other', 'best', 'many', 'first', 'more', 'little', 'much', 'real', 'new', 'young', 'old', 'such', 'most', 'same', 'own', 'few', 'bad', 'big', 'funny', 'excellent', 'last', 'different', 'better', 'true', 'beautiful', 'original', 'wonderful', 'only', 'american', 'whole', 'ive', 'hard', 'least', 'nice', 'high', 'perfect', 'interesting', 'main', 'classic', 'long', 'small', 'special', 'sure', 'black', 'human', 'full', 'short', 'second', 'favorite', 'early', 'top', 'fine', 'right', 'several', 'brilliant', 'next', 'white', 'strong', 'final', 'able', 'dead', 'overall', 'fantastic', 'simple', 'wrong', 'amazing', 'hilarious', 'late', 'entire', 'happy', 'musical', 'low', 'enjoyable', 'worth', 'romantic', 'important', 'modern', 'usual', 'greatest', 'british', 'less', 's', 'easy', 'realistic', 'serious', 'entertaining', 'similar', 'enough', 'dark', 'powerful', 'english', 'im', 'comic', 'french', 'due', 'memorable', 'local', 'emotional', 'dont', 'certain', 'unique', 'famous', 'major', 'evil', 'huge', 'possible', 'believable', 'typical', 'sad', 'particular', 'strange', 'poor', 'personal', 'difficult', 'incredible', 'japanese', 'red', 'obvious', 'slow', 'cant', 'dramatic', 'single', 'political', 'older', 'cool', 'general', 'social', 'third', 'sexual', 'actual', 'various', 'clear', 'solid', 'past', 'deep', 'outstanding', 'popular', 'sweet', 'pretty', 'natural', 'effective', 'rare', 'previous', 'rich', 'familiar', 'scary', 'western']\n",
      "['i', 'bad', 'good', 'other', 'first', 'more', 'much', 'many', 'little', 'great', 'worst', 'old', 'real', 'better', 'few', 'only', 'such', 'same', 'least', 'original', 'funny', 'new', 'best', 'most', 'big', 'whole', 'poor', 'ive', 'awful', 'last', 'stupid', 'terrible', 'young', 'own', 'main', 'low', 'hard', 'sure', 'special', 'worse', 'interesting', 'wrong', 'high', 'dead', 'black', 'horrible', 'american', 'dont', 'long', 'second', 'entire', 'different', 'nice', 'true', 'short', 'ridiculous', 'next', 'full', 'decent', 'white', 'boring', 'small', 'im', 'right', 'obvious', 'enough', 'several', 'predictable', 'cheap', 'final', 'complete', 'classic', 'scary', 'less', 'cant', 'beautiful', 'slow', 'single', 'early', 'human', 'worth', 'able', 'huge', 'possible', 'weak', 'evil', 'overall', 's', 'top', 'pretty', 'serious', 'dull', 'major', 'actual', 'usual', 'sad', 'local', 'pathetic', 'dumb', 'english', 'total', 'lame', 'strange', 'due', 'flat', 'female', 'fine', 'late', 'ok', 'happy', 'non', 'excellent', 'laughable', 'hilarious', 'important', 'general', 'cool', 'british', 'clear', 'sexual', 'comic', 'musical', 'modern', 'free', 'disappointed', 'hot', 'similar', 'typical', 'unbelievable', 'dark', 'strong', 'positive', 'certain', 'silly', 'third', 'red', 'previous', 'perfect', 'japanese', 'biggest', 'average', 'painful', 'weird', 'difficult', 'simple', 'famous', 'potential', 'french', 'dramatic', 'half']\n"
     ]
    }
   ],
   "source": [
    "#give the adjectives appearing in the data\n",
    "import nltk\n",
    "def findAdjectiveWords(data, n):\n",
    "    counter = dict()\n",
    "    for item in data['text']:\n",
    "        word = item.lower().split()\n",
    "        for each, tag in nltk.pos_tag(word):\n",
    "            if tag==\"JJ\" or tag==\"JJR\" or tag==\"JJS\":\n",
    "                if each in counter:\n",
    "                    counter[each]+=1\n",
    "                else:\n",
    "                    counter[each]=1\n",
    "    \n",
    "    topWordsAndCount = [(k, counter[k]) for k in sorted(counter, key=counter.get, reverse=True)]\n",
    "    topWords = [word[0] for word in topWordsAndCount]\n",
    "    topnWords = topWords[0:n]\n",
    "             \n",
    "    print(topnWords)\n",
    "    return topnWords\n",
    "\n",
    "goodWord = findAdjectiveWords(trainPos, 150)\n",
    "badWord = findAdjectiveWords(trainNeg, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dead', 'same', 'japanese', 'black', 'able', 'old', 'boring', 'laughable', 'rich', 'true', 'top', 'usual', 'difficult', 'positive', 'favorite', 'lame', 'ok', 'simple', 'such', 'red', 'worst', 'better', 'beautiful', 'dramatic', 'easy', 'amazing', 'nice', 'general', 'female', 'sure', 'big', 'wrong', 'hilarious', 'huge', 'hot', 'particular', 'many', 'poor', 's', 'political', 'potential', 'classic', 'fantastic', 'comic', 'last', 'decent', 'pathetic', 'enjoyable', 'several', 'french', 'brilliant', 'perfect', 'right', 'effective', 'half', 'original', 'great', 'long', 'stupid', 'weak', 'obvious', 'disappointed', 'scary', 'typical', 'cool', 'certain', 'unique', 'bad', 'personal', 'serious', 'previous', 'final', 'pretty', 'awful', 'outstanding', 'third', 'complete', 'wonderful', 'flat', 'own', 'important', 'terrible', 'strange', 'least', 'various', 'only', 'romantic', 'happy', 'weird', 'high', 'free', 'different', 'predictable', 'i', 'sad', 'im', 'ive', 'overall', 'interesting', 'strong', 'possible', 'deep', 'main', 'musical', 'realistic', 'believable', 'solid', 'clear', 'much', 'small', 'next', 'greatest', 'low', 'silly', 'single', 'familiar', 'whole', 'natural', 'emotional', 'short', 'english', 'funny', 'biggest', 'average', 'similar', 'horrible', 'new', 'young', 'other', 'actual', 'full', 'western', 'due', 'unbelievable', 'modern', 'real', 'late', 'painful', 'past', 'best', 'most', 'first', 'worse', 'dumb', 'memorable', 'more', 'dont', 'entertaining', 'british', 'rare', 'powerful', 'evil', 'excellent', 'sexual', 'little', 'dull', 'local', 'good', 'white', 'fine', 'sweet', 'few', 'cheap', 'entire', 'popular', 'less', 'second', 'american', 'older', 'dark', 'major', 'famous', 'worth', 'non', 'incredible', 'hard', 'social', 'total', 'human', 'cant', 'slow', 'ridiculous', 'special', 'early', 'enough']\n"
     ]
    }
   ],
   "source": [
    "allWords = list(set(goodWord).union(set(badWord)))\n",
    "print(allWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the features as binary, check if the word is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAll = pd.concat([trainPos[0:10000], trainNeg[0:10000]])\n",
    "validationAll = pd.concat([trainPos[10000:11250], trainNeg[10000:11250]])\n",
    "\n",
    "def featuresConstruction(data, allWords):\n",
    "    adjFeatures=[]\n",
    "    for text in data['text']:\n",
    "        adjFeature = dict()\n",
    "        for word in allWords:\n",
    "            if word in text:\n",
    "                adjFeature[word]=1\n",
    "            else:\n",
    "                adjFeature[word]=0\n",
    "        temp = [adjFeature[k] for k in adjFeature]\n",
    "        adjFeatures.append(temp)\n",
    "\n",
    "    adjFeatures = np.array(adjFeatures)\n",
    "    return adjFeatures\n",
    "\n",
    "adjFeatures = featuresConstruction(trainAll, allWords)\n",
    "validationFeatures = featuresConstruction(validationAll, allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDiscrete = np.atleast_2d(trainAll[\"y discrete\"]).T\n",
    "yValidation = validationAll[\"y discrete\"]\n",
    "yValidation = np.array(yValidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function takes in the feature matrix and the response matrix\n",
    "def BernoulliNaiveBayes(X, y):\n",
    "    dataSize = y.size\n",
    "    classCount = dict()\n",
    "    classCount[\"1\"] = sum(y) # count of y=1\n",
    "    classCount[\"0\"] = dataSize - classCount[\"1\"] # count of y=0\n",
    "    priorProb = {k: v / dataSize for k, v in classCount.items()} # P(y=1) and P(y=0)\n",
    "\n",
    "    XandY=np.concatenate((X,y), axis = 1)\n",
    "    \n",
    "    \n",
    "    eachTotalCount = sum(X) #the total number of counts for each word\n",
    "    positiveRow = []\n",
    "    negativeRow = []\n",
    "    \n",
    "    for row in XandY:\n",
    "        if row[XandY.shape[1]-1]==1:\n",
    "            positiveRow.append(row)\n",
    "        else:\n",
    "            negativeRow.append(row)\n",
    "    positiveRow = np.delete(np.array(positiveRow), -1, axis=1)\n",
    "\n",
    "    \n",
    "    # calculate the counts of words in positive set and negative set\n",
    "    presentPosCount = sum(positiveRow) #number of instance with xj=1 and y=1\n",
    "    presentNegCount = eachTotalCount - presentPosCount #number of instance with xj=1 and y=0\n",
    "    \n",
    "    # the probability after laplace smoothing\n",
    "    presentPosProb = (presentPosCount+1)/(classCount[\"1\"]+2) #P(xj=1|y=1)\n",
    "    presentNegProb = (presentNegCount+1)/(classCount[\"0\"]+2) #P(xj=1|y=0)\n",
    "    absentPosProb = 1-presentPosProb #P(xj=0|y=1)\n",
    "    absentNegProb = 1-presentNegProb #P(xj=0|y=0)\n",
    "    \n",
    "    w_j0 = np.log(np.divide(absentPosProb, absentNegProb))\n",
    "    w_j1 = np.log(np.divide(presentPosProb, presentNegProb))  \n",
    "    \n",
    "    \n",
    "    w0 = np.log(np.divide(priorProb[\"1\"], priorProb[\"0\"]))+sum(w_j0)\n",
    "    w = w_j1 - w_j0\n",
    "    \n",
    "    return w0, w\n",
    "\n",
    "\n",
    "w0, w = BernoulliNaiveBayes(adjFeatures, yDiscrete)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w0, w, validationX):\n",
    "    prediction = []\n",
    "    for row in validationX:\n",
    "        result = w0+np.dot(row.T, w)\n",
    "        if result>0:\n",
    "            prediction.append(\"1\")\n",
    "        else:\n",
    "            prediction.append(\"0\")\n",
    "    return prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(w0, w, validationFeatures)\n",
    "prediction = np.array(prediction)\n",
    "prediction.shape\n",
    "prediction.shape\n",
    "yValidation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.810875649740104"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "prediction = [int(x) for x in prediction]\n",
    "f1_score(yValidation,prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
